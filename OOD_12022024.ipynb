{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mtlukasik/Exploration/blob/main/OOD_12022024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TigNdgTD0qqo"
      },
      "source": [
        "## Import libraries\n",
        "\n",
        "- Import additional libraries of your choice.\n",
        "\n",
        "Although you are expected to demonstrate understanding of ML/DS/statistics tools, a particular choice of libraries and frameworks will not affect evaluation of the solution."
      ],
      "id": "TigNdgTD0qqo"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "def check_calibration(dataset,predicted_probs):\n",
        "    x_vals = []\n",
        "    y_vals = []\n",
        "    # Since we can't directly iterate i and j in pairs with range, let's define j as i + step within the loop.\n",
        "    step = 0.01\n",
        "    for i in np.arange(0, 1, step):\n",
        "      try:\n",
        "          j = i + step  # Defining j based on the step\n",
        "          # Finding indices where probs meet the condition between i and j\n",
        "          indices = np.where(np.logical_and(predicted_probs.flatten() >= i, predicted_probs.flatten() <= j))[0]\n",
        "          a = dataset['install'].values[indices]\n",
        "          if len(a)>0:\n",
        "            x_vals.append((i+j)/2)\n",
        "            y_vals.append(sum(a)/len(a))\n",
        "      except ZeroDivisionError:\n",
        "        continue\n",
        "    # Plotting\n",
        "    plt.plot(x_vals,x_vals)\n",
        "    plt.scatter(x_vals, y_vals)\n",
        "    plt.xlabel('(i + j) / 2')\n",
        "    plt.ylabel('Mean of Selected Install Values')\n",
        "    plt.title('Scatter Plot of Mean Install Values')\n",
        "    plt.show()\n",
        "#check_calibration(TRAIN_DATA,probs)\n",
        "\n",
        "class OODModel:\n",
        "\n",
        "    def __init__(self, input_shape):\n",
        "        self.model = self._build_model(input_shape)\n",
        "        self.target_column = 'OOD'\n",
        "\n",
        "    def _build_model(self, input_shape):\n",
        "        # Define a simple Sequential model\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(64, activation='relu', input_shape=(input_shape,)),\n",
        "            tf.keras.layers.Dropout(0.1),\n",
        "            tf.keras.layers.Dense(64, activation='relu'),\n",
        "            tf.keras.layers.Dropout(0.1),\n",
        "            tf.keras.layers.Dense(1, activation='sigmoid')  # Assuming binary classification\n",
        "        ])\n",
        "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    def fit(self, X, y, epochs=2, validation_split=0.2):\n",
        "        self.model.fit(X, y, epochs=epochs, validation_split=validation_split)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(X)"
      ],
      "metadata": {
        "id": "Oip4jZsmNxiu"
      },
      "id": "Oip4jZsmNxiu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4FMNn8o0qqp"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "import seaborn as sns\n",
        "import keras\n",
        "import sklearn\n",
        "import hashlib\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats as stats\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from keras.layers import Input, Dense, Reshape\n",
        "# Import additional libraries of your choice"
      ],
      "id": "A4FMNn8o0qqp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeYfD33i0qqk"
      },
      "source": [
        "# Unity Data Science quiz\n",
        "\n",
        "At Unity, we develop deep learning models for real-time ads bidding ([OpenRTB](https://www.iab.com/guidelines/openrtb/)) at various ad\n",
        "exchanges. To bid for an ad impression, we estimate the optimal bid value using predicted\n",
        "install probability of campaign together with several other factors e.g. cost per install.\n",
        "\n",
        "In this homework, your task is to **train a model to predict *install probabilities* for ad impressions included in\n",
        "the test data**. In the production environment, the model predictions are used for deriving the optimial bids for available ad campaigns.  The best ad campaign will be shown to the user. Overestimation of install probabilities will lead to unnecessarily high bids and monetary losses, while underestimation of install probabilities will lead to unnecessarily low bids and loss of opportunities for Unity to win ad impressions. Therefore, it is important for the model predictions to be as accurate as possible.\n",
        "\n",
        "## Instructions\n",
        "\n",
        "- Complete the homework using Python and libraries of your choice.\n",
        "- Follow the instructions in this notebook.\n",
        "- Keep the code clean and organized.\n",
        "\n",
        "## Evaluation\n",
        "\n",
        "We focus our evaluation on technical proficiency, analytical skills, problem solving, creative thinking as well as ability to communicate clearly. In particular, we will evaluate:\n",
        "- understanding of the problem (e.g. does a delivered solution meet the specification of the task)\n",
        "- quality of discussion and brevity of the report (e.g. comments in this notebook)\n",
        "- quality of EDA\n",
        "- feature handling & preprocessing\n",
        "- modeling approach\n",
        "- model validation and evaluation\n",
        "\n",
        "We will separately evaluate the predicted test set install probabilities (see below). Although considered as part of the evaluation, the final performance is not the key factor and shall not dominate over the above dimensions.\n",
        "\n",
        "## Data description\n",
        "\n",
        "- ```id```: impression id\n",
        "- ```timestamp```: time of the event in UTC ```\n",
        " all installs happened  long ago this game is probably old```\n",
        "- ```campaignId```: id of the advertising campaign (the game being advertised)\n",
        "- ```platform```: device platform\n",
        "- ```softwareVersion```: OS version of the device\n",
        "- ```country```: country of user\n",
        "- ```sourceGameId```: id of the publishing game (the game being played)\n",
        "- ```startCount```: how many times the user has started (any) campaigns\n",
        "- ```viewCount```: how many times the user has viewed (any) campaigns\n",
        "- ```clickCount```: how many times the user has clicked (any) campaigns\n",
        "- ```installCount```: how many times the user has installed games from this ad network\n",
        "- ```lastStart```: last time the user started any campaign\n",
        "- ```startCount1d```: how many times the user has started (any) campaigns within the last 24 hours\n",
        "- ```startCount7d```: how many times the user has started (any) campaigns within the last 7 days\n",
        "- ```connectionType```: internet connection type\n",
        "- ```deviceType```: device model\n",
        "- ```install```: binary indicator if install was observed (install=1) or not (install=0) after impression\n",
        "\n",
        "## Submission\n",
        "\n",
        "- This Jupyter notebook\n",
        "- A CSV file containing the predicted install probabilities of ad impressions in the test data. The file should have the following columns:\n",
        "    - ```id```: ID of ad impression in the test data\n",
        "    - ```install_proba```: Predicted install probability of ad impression\n"
      ],
      "id": "LeYfD33i0qqk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6m_k3Um_0qqq"
      },
      "source": [
        "## Load and prepare data"
      ],
      "id": "6m_k3Um_0qqq"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "train_df = pd.read_csv(\"/content/drive/MyDrive/training_data.csv\", sep=\";\", parse_dates=True)\n",
        "train_df = train_df[~train_df['install'].isna()]#inputing install label is classification so there is no use in it\n",
        "train_df = train_df[~train_df['lastStart'].isna()]\n",
        "train_df['lastStart'] = pd.to_datetime(train_df['lastStart'])\n",
        "train_df = train_df.sort_values(by=['timestamp']).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "Bc8BuoLlfKnr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "169d19a3-d813-4856-9f53-48a43f4af2e3"
      },
      "id": "Bc8BuoLlfKnr",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Ensure 'timestamp' is in datetime format\n",
        "train_df['timestamp'] = pd.to_datetime(train_df['timestamp'])\n",
        "daily_groups = train_df.groupby(pd.Grouper(key='timestamp', freq='D'))\n",
        "daily_dfs = []\n",
        "for _, group in daily_groups:\n",
        "    if not group.empty:  # Check if the group is not empty\n",
        "        daily_dfs.append(group)\n",
        "[i.shape for i in daily_dfs]"
      ],
      "metadata": {
        "id": "8CwLX3FteOUU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd65808d-4db1-4c70-b4c4-3a4806d12d67"
      },
      "id": "8CwLX3FteOUU",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(315482, 17),\n",
              " (313690, 17),\n",
              " (306825, 17),\n",
              " (281678, 17),\n",
              " (232467, 17),\n",
              " (225278, 17),\n",
              " (189106, 17),\n",
              " (201060, 17),\n",
              " (214095, 17),\n",
              " (211327, 17),\n",
              " (223277, 17),\n",
              " (257102, 17),\n",
              " (272655, 17),\n",
              " (219667, 17)]"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "daily_unseen_dfs = [df[df['country'] == 'US'] for df in daily_dfs[:7]]\n",
        "daily_modified_dfs = []\n",
        "\n",
        "for df in daily_dfs[:7]:\n",
        "    # Remove rows where 'country' == 'US' and add to the new list\n",
        "    daily_modified_dfs.append(df[df['country'] != 'US'])"
      ],
      "metadata": {
        "id": "ai_w03qS4JUS"
      },
      "id": "ai_w03qS4JUS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seen_df = pd.concat(daily_modified_dfs)\n",
        "unseen_df = pd.concat(daily_unseen_dfs)"
      ],
      "metadata": {
        "id": "lQ-QP_fK59ln"
      },
      "id": "lQ-QP_fK59ln",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_DATA = seen_df\n",
        "UNSEEN_DATA = unseen_df\n",
        "TRAFFIC_DATA = daily_dfs[8]"
      ],
      "metadata": {
        "id": "1nbC57nnoeD3"
      },
      "id": "1nbC57nnoeD3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_DATA.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "j27n8eBedB0N",
        "outputId": "bdb8487b-f2a8-46b8-f3c6-fe97c4271c89"
      },
      "id": "j27n8eBedB0N",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                         id                        timestamp  \\\n",
              "0  5c33e8004eeff537f7cab9af 2019-01-08 00:00:00.368000+00:00   \n",
              "2  5c33e8005b62743723aab607 2019-01-08 00:00:00.752000+00:00   \n",
              "3  5c33e800ffa6e3378d7b4ffd 2019-01-08 00:00:00.795000+00:00   \n",
              "5  5c33e801faa1d437f73878a9 2019-01-08 00:00:01.857000+00:00   \n",
              "6  5c33e8026922e637c2ceae6e 2019-01-08 00:00:02.094000+00:00   \n",
              "\n",
              "                 campaignId platform softwareVersion  sourceGameId country  \\\n",
              "0  5c26e5222127049b4bce982d  android           8.0.0       1432611      NZ   \n",
              "2  5be28ecbc621317c947a63c7      ios          12.1.2       2612974      FR   \n",
              "3  5af41f7dce114315dc22d94d  android           6.0.1       2626028      BR   \n",
              "5  5b42ff00354f8159b6a49b15  android           6.0.1         61378      AR   \n",
              "6  5c2ed1c592f69c5f56a4c852      ios          12.1.2         20721      MK   \n",
              "\n",
              "   startCount  viewCount  clickCount  installCount  \\\n",
              "0          44         41           0             2   \n",
              "2         129        124           1             1   \n",
              "3          11          7           0             0   \n",
              "5          21         13           2             0   \n",
              "6          41         40           3             0   \n",
              "\n",
              "                         lastStart  startCount1d  startCount7d connectionType  \\\n",
              "0 2019-01-07 20:50:38.893000+00:00             5            12           wifi   \n",
              "2 2019-01-07 23:59:27.976000+00:00             6            31           wifi   \n",
              "3 2019-01-07 13:19:03.738000+00:00             4             4           wifi   \n",
              "5 2019-01-07 23:59:25.936000+00:00             4            19           wifi   \n",
              "6 2019-01-07 23:57:47.384000+00:00             2            15           wifi   \n",
              "\n",
              "         deviceType  install  \n",
              "0  samsung SM-G935F        0  \n",
              "2        iPhone10,6        0  \n",
              "3  samsung SM-J500M        0  \n",
              "5  samsung SM-G532M        0  \n",
              "6        iPhone10,2        0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-55c0098a-0fc3-4e53-82d1-08d1a377649b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>campaignId</th>\n",
              "      <th>platform</th>\n",
              "      <th>softwareVersion</th>\n",
              "      <th>sourceGameId</th>\n",
              "      <th>country</th>\n",
              "      <th>startCount</th>\n",
              "      <th>viewCount</th>\n",
              "      <th>clickCount</th>\n",
              "      <th>installCount</th>\n",
              "      <th>lastStart</th>\n",
              "      <th>startCount1d</th>\n",
              "      <th>startCount7d</th>\n",
              "      <th>connectionType</th>\n",
              "      <th>deviceType</th>\n",
              "      <th>install</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5c33e8004eeff537f7cab9af</td>\n",
              "      <td>2019-01-08 00:00:00.368000+00:00</td>\n",
              "      <td>5c26e5222127049b4bce982d</td>\n",
              "      <td>android</td>\n",
              "      <td>8.0.0</td>\n",
              "      <td>1432611</td>\n",
              "      <td>NZ</td>\n",
              "      <td>44</td>\n",
              "      <td>41</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2019-01-07 20:50:38.893000+00:00</td>\n",
              "      <td>5</td>\n",
              "      <td>12</td>\n",
              "      <td>wifi</td>\n",
              "      <td>samsung SM-G935F</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5c33e8005b62743723aab607</td>\n",
              "      <td>2019-01-08 00:00:00.752000+00:00</td>\n",
              "      <td>5be28ecbc621317c947a63c7</td>\n",
              "      <td>ios</td>\n",
              "      <td>12.1.2</td>\n",
              "      <td>2612974</td>\n",
              "      <td>FR</td>\n",
              "      <td>129</td>\n",
              "      <td>124</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2019-01-07 23:59:27.976000+00:00</td>\n",
              "      <td>6</td>\n",
              "      <td>31</td>\n",
              "      <td>wifi</td>\n",
              "      <td>iPhone10,6</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5c33e800ffa6e3378d7b4ffd</td>\n",
              "      <td>2019-01-08 00:00:00.795000+00:00</td>\n",
              "      <td>5af41f7dce114315dc22d94d</td>\n",
              "      <td>android</td>\n",
              "      <td>6.0.1</td>\n",
              "      <td>2626028</td>\n",
              "      <td>BR</td>\n",
              "      <td>11</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2019-01-07 13:19:03.738000+00:00</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>wifi</td>\n",
              "      <td>samsung SM-J500M</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5c33e801faa1d437f73878a9</td>\n",
              "      <td>2019-01-08 00:00:01.857000+00:00</td>\n",
              "      <td>5b42ff00354f8159b6a49b15</td>\n",
              "      <td>android</td>\n",
              "      <td>6.0.1</td>\n",
              "      <td>61378</td>\n",
              "      <td>AR</td>\n",
              "      <td>21</td>\n",
              "      <td>13</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2019-01-07 23:59:25.936000+00:00</td>\n",
              "      <td>4</td>\n",
              "      <td>19</td>\n",
              "      <td>wifi</td>\n",
              "      <td>samsung SM-G532M</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>5c33e8026922e637c2ceae6e</td>\n",
              "      <td>2019-01-08 00:00:02.094000+00:00</td>\n",
              "      <td>5c2ed1c592f69c5f56a4c852</td>\n",
              "      <td>ios</td>\n",
              "      <td>12.1.2</td>\n",
              "      <td>20721</td>\n",
              "      <td>MK</td>\n",
              "      <td>41</td>\n",
              "      <td>40</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2019-01-07 23:57:47.384000+00:00</td>\n",
              "      <td>2</td>\n",
              "      <td>15</td>\n",
              "      <td>wifi</td>\n",
              "      <td>iPhone10,2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-55c0098a-0fc3-4e53-82d1-08d1a377649b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-55c0098a-0fc3-4e53-82d1-08d1a377649b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-55c0098a-0fc3-4e53-82d1-08d1a377649b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-55c979f8-4c8f-4f75-8941-97fcadf7244a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-55c979f8-4c8f-4f75-8941-97fcadf7244a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-55c979f8-4c8f-4f75-8941-97fcadf7244a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "TRAIN_DATA"
            }
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "\n",
        "class SampleWeightNormalizer:\n",
        "    @staticmethod\n",
        "    def reweight(sample_weight: Optional[tf.Tensor]) -> Optional[tf.Tensor]:\n",
        "\n",
        "        if sample_weight is not None:\n",
        "            sample_weight = (\n",
        "                sample_weight\n",
        "                * tf.cast(tf.shape(sample_weight)[0], tf.float32)\n",
        "                / tf.reduce_sum(sample_weight, axis=0)\n",
        "            )\n",
        "\n",
        "        return sample_weight\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "class WeightedBCELossWrapper(tf.keras.losses.BinaryCrossentropy, SampleWeightNormalizer):\n",
        "    def __init__(self, y_pred_idx=None, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        # ensure only the sampled branch is used in loss calculation\n",
        "        # y_pred_idx variable specifies the index of the branch (mean or sampled) to be used in the loss wrapper\n",
        "        self.y_pred_idx = y_pred_idx\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        y_true: tf.Tensor,\n",
        "        y_pred: tf.Tensor,\n",
        "        sample_weight: Optional[tf.Tensor] = None,\n",
        "    ) -> Optional[tf.Tensor]:\n",
        "\n",
        "        if self.y_pred_idx is not None:\n",
        "            y_pred = tf.gather(y_pred, self.y_pred_idx, axis=-1)\n",
        "\n",
        "        return super().__call__(\n",
        "            y_true=y_true,\n",
        "            y_pred=y_pred,\n",
        "            sample_weight=self.reweight(sample_weight),\n",
        "        )\n",
        "\n",
        "    def get_config(self):\n",
        "        \"\"\"Method that returns a dictionary with its key-value pair representing the kwargs in `self`\"\"\"\n",
        "        return {}\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "class WeightedBCEMetricWrapper(tf.keras.metrics.BinaryCrossentropy, SampleWeightNormalizer):\n",
        "\n",
        "    def __init__(self, y_pred_idx=None, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        # ensure only the sampled branch is used in loss metric calculation\n",
        "        self.y_pred_idx = y_pred_idx\n",
        "\n",
        "    def update_state(\n",
        "        self,\n",
        "        y_true: tf.Tensor,\n",
        "        y_pred: tf.Tensor,\n",
        "        sample_weight: Optional[tf.Tensor] = None,\n",
        "    ) -> Optional[tf.Tensor]:\n",
        "\n",
        "        if self.y_pred_idx is not None:\n",
        "            y_pred = tf.gather(y_pred, self.y_pred_idx, axis=-1)\n",
        "\n",
        "        return super().update_state(\n",
        "            y_true=y_true,\n",
        "            y_pred=y_pred,\n",
        "            sample_weight=self.reweight(sample_weight),\n",
        "        )\n",
        "\n",
        "    def get_config(self):\n",
        "        \"\"\"Method that returns a dictionary with its key-value pair representing the kwargs in `self`\"\"\"\n",
        "        return {}\n",
        "import pandas as pd\n",
        "from sklearn.utils import resample\n",
        "\n",
        "\n",
        "class DataPreprocessor:\n",
        "\n",
        "    def __init__(self, numerical_columns, target_column, categorical_columns=None):\n",
        "        self.numerical_columns = numerical_columns\n",
        "        self.target_column = target_column\n",
        "        # Initialize with an empty list if no categorical columns are provided\n",
        "        self.categorical_columns = categorical_columns if categorical_columns is not None else []\n",
        "        # Initialize mappings only if there are categorical columns\n",
        "        self.cat_col_mappings = {col: {} for col in self.categorical_columns}\n",
        "\n",
        "    def fit_categorical_mappings(self, df):\n",
        "        # Proceed only if there are categorical columns\n",
        "        if self.categorical_columns:\n",
        "            for col in self.categorical_columns:\n",
        "                value_counts = df[col].value_counts()\n",
        "                valid_values = value_counts[value_counts > 100].index.tolist()\n",
        "                self.cat_col_mappings[col] = {v: i+1 for i, v in enumerate(valid_values)}\n",
        "\n",
        "    def map_categorical_values(self, df):\n",
        "        # Map values only if there are categorical columns\n",
        "        if self.categorical_columns:\n",
        "            for col in self.categorical_columns:\n",
        "                df[col] = df[col].apply(lambda x: self.cat_col_mappings[col].get(x, 0))\n",
        "        return df\n",
        "\n",
        "    def prepreprocess_(self, df):\n",
        "        self.fit_categorical_mappings(df)\n",
        "\n",
        "        # Include categorical columns in processing only if they exist\n",
        "        columns = self.numerical_columns + (self.categorical_columns if self.categorical_columns else [])\n",
        "        if self.target_column in df.columns:\n",
        "            columns += [self.target_column]\n",
        "        processed_df = df[columns].copy()\n",
        "\n",
        "        # Convert numerical columns to float\n",
        "        for col in self.numerical_columns:\n",
        "            processed_df[col] = pd.to_numeric(processed_df[col], errors='coerce').astype(float)\n",
        "\n",
        "        # Map categorical columns to IDs, if any\n",
        "        processed_df = self.map_categorical_values(processed_df)\n",
        "        return processed_df\n",
        "\n",
        "    def preprocess_(self, df, return_xy=True, fit_cat_mappings=False):\n",
        "        processed_df = self.prepreprocess_(df)\n",
        "\n",
        "        if return_xy:\n",
        "            processed_df, y, weights = self.preprocess_train(processed_df)\n",
        "        # Include categorical columns in X_train if they exist\n",
        "        X_train = {col + \"_input\": processed_df[col].values for col in self.numerical_columns+self.categorical_columns}\n",
        "\n",
        "        if return_xy:\n",
        "            return X_train, y, weights\n",
        "        else:\n",
        "            return X_train\n",
        "\n",
        "    # Your existing preprocess_train method remains unchanged\n",
        "    def preprocess_train(self, df):\n",
        "        # Existing implementation\n",
        "\n",
        "        label_counts = df[self.target_column].value_counts()\n",
        "        majority_label = label_counts.idxmax()\n",
        "        minority_label = label_counts.idxmin()\n",
        "\n",
        "        df_majority = df[df[self.target_column] == majority_label]\n",
        "        df_minority = df[df[self.target_column] == minority_label]\n",
        "\n",
        "        df_majority_downsampled = resample(df_majority, replace=False, n_samples=len(df_minority), random_state=123)\n",
        "        balanced_df = pd.concat([df_minority, df_majority_downsampled])\n",
        "        balanced_df = balanced_df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "        # Now, you can convert this balanced_df into X_train, y format or continue using it as a DataFrame for further processing.\n",
        "        X_train = balanced_df[balanced_df.columns.difference([self.target_column])]\n",
        "        y = pd.to_numeric(balanced_df[self.target_column], errors='coerce').astype('float32').values\n",
        "        sampling_ratio = len(df_minority) / (len(df_minority) + len(df_majority))\n",
        "        weights = balanced_df[self.target_column].apply(\n",
        "            lambda x: 1 if x == minority_label else 1 / sampling_ratio\n",
        "        ).values\n",
        "\n",
        "        return X_train, y, weights\n",
        "\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable(package=\"Custom\", name=\"PermanentDropout\")\n",
        "class PermanentDropout(tf.keras.layers.Dropout):\n",
        "    def call(self, inputs, training=None):\n",
        "        return super().call(inputs, training=True)\n",
        "\n",
        "class ConversionModel:\n",
        "\n",
        "    def __init__(self, input_shape,numerical_columns,categorical_columns,cat_col_mappings):\n",
        "        self.continuous_feats = numerical_columns\n",
        "        self.categorical_feats = categorical_columns\n",
        "        self.target_column = 'install'\n",
        "        self.CPILoader = types.SimpleNamespace()\n",
        "        self.CPILoader.cat_col_mappings = cat_col_mappings\n",
        "\n",
        "        self.neural_network_structure = {   'add_batchnorm': True, 'input_dropout': 0.0,'n_layers': 3,\n",
        "            '0':{'activation': 'swish',\n",
        "              'size': 512/8,\n",
        "              'dropout': 0.2},\n",
        "            '1':{'activation': 'swish',\n",
        "              'size': 256/8,\n",
        "              'dropout': 0.1},\n",
        "            '2':{'activation': 'swish',\n",
        "              'size': 128/8,\n",
        "              'dropout': 0.06},\n",
        "            'hidden_l1': 0.0,\n",
        "            'hidden_l2': 0.0}\n",
        "        self.model = self._build_model(input_shape)\n",
        "    def build_dnn_two_towers_with_shared_weights(self):\n",
        "          \"\"\"\n",
        "          creates 2 identical towers with shared weights\n",
        "          one of the towers is used for mean prediction, the other for sampling\n",
        "          the mc dropout is applied only to the sampled tower\n",
        "          \"\"\"\n",
        "          mean_dnn_layers = []\n",
        "          sampling_dnn_layers = []\n",
        "          for i in range(self.neural_network_structure[\"n_layers\"]):\n",
        "              config = self.neural_network_structure[str(i)]\n",
        "\n",
        "              dense_units = config[\"size\"]\n",
        "              activation = config[\"activation\"]\n",
        "              dropout_rate = config[\"dropout\"]\n",
        "\n",
        "\n",
        "              dense_layer = Dense(\n",
        "                  dense_units,\n",
        "                  kernel_regularizer=tf.keras.regularizers.L1L2(\n",
        "                      l1=self.neural_network_structure.get(\"hidden_l1\", 0.0),\n",
        "                      l2=self.neural_network_structure.get(\"hidden_l2\", 0.0),\n",
        "                  ),\n",
        "                  name=f\"hidden_{i}\",\n",
        "              )\n",
        "\n",
        "              layers = [\n",
        "                  dense_layer,\n",
        "                  tf.keras.layers.BatchNormalization() if self.neural_network_structure.get(\"add_batchnorm\") else None,\n",
        "                  tf.keras.layers.Activation(activation),\n",
        "              ]\n",
        "\n",
        "              mean_dnn_layers += [k for k in layers if k is not None]+ [tf.keras.layers.Dropout(dropout_rate)]\n",
        "              sampling_dnn_layers += [k for k in layers if k is not None] + [PermanentDropout(dropout_rate)]\n",
        "\n",
        "          return (\n",
        "              tf.keras.models.Sequential(mean_dnn_layers, name=\"mean_deep_neural_network\"),\n",
        "              tf.keras.models.Sequential(sampling_dnn_layers, name=\"sampling_deep_neural_network\"),\n",
        "              )\n",
        "    def embedding_input(self,\n",
        "        name,\n",
        "        n_in,\n",
        "        n_out,\n",
        "        initializer_name=\"truncated_normal\",\n",
        "        l1=0.0,\n",
        "        l2=0.0,\n",
        "    ):\n",
        "        inp = Input(shape=(1,), dtype=\"int64\", name=name + \"_input\")\n",
        "\n",
        "        if initializer_name == \"uniform\":\n",
        "            initializer = tf.keras.initializers.RandomUniform(minval=-1.0, maxval=1.0)\n",
        "        elif initializer_name == \"truncated_normal\":\n",
        "            initializer = tf.keras.initializers.TruncatedNormal(mean=0, stddev=1 / np.sqrt(n_in))\n",
        "        else:\n",
        "            raise ValueError(\"ERROR: Trying to initialize embeddings with unknown initializer.\")\n",
        "\n",
        "        regularizer = tf.keras.regularizers.L1L2(l1=l1, l2=l2) if l1 > 0.0 or l2 > 0.0 else None\n",
        "\n",
        "        return inp, tf.keras.layers.Embedding(\n",
        "            n_in,\n",
        "            n_out,\n",
        "            input_length=1,\n",
        "            embeddings_initializer=initializer,\n",
        "            embeddings_regularizer=regularizer,\n",
        "            name=name + \"_embedding\",\n",
        "        )(inp)\n",
        "\n",
        "    def build_inputs(self,category_mappings):\n",
        "        def continuous_input(name, preprocessed_key=None):\n",
        "          input_name = preprocessed_key or name + \"_input\"\n",
        "          inp = Input(shape=(1,), dtype=\"float32\", name=input_name)\n",
        "          return inp, Reshape((1, 1), name=name + \"_reshape\")(inp)\n",
        "        inp_layer = []\n",
        "        inp_embed = []\n",
        "        if self.categorical_feats:\n",
        "          for cat_col in self.categorical_feats:\n",
        "              embed_dim = 10\n",
        "              n_in = len(category_mappings[cat_col])+1\n",
        "              t_inp, t_build = self.embedding_input(\n",
        "                  cat_col,n_in,n_out=embed_dim)\n",
        "              if self.neural_network_structure.get(\"add_batchnorm\"):\n",
        "                t_build = tf.keras.layers.BatchNormalization()(t_build)\n",
        "\n",
        "              inp_layer.append(t_inp)\n",
        "              inp_embed.append(t_build)\n",
        "        for con_col in self.continuous_feats:\n",
        "            t_inp, t_build = continuous_input(con_col)\n",
        "            inp_layer.append(t_inp)\n",
        "            inp_embed.append(t_build)\n",
        "        return inp_layer, inp_embed\n",
        "\n",
        "    def _build_model(self, input_shape):\n",
        "        input_layer, inputs = self.build_inputs(self.CPILoader.cat_col_mappings)\n",
        "        #print(f\"Concatenated Inputs Shape: {len(inputs)},{inputs[0].shape},{inputs[1].shape}\")\n",
        "\n",
        "        #print(self.CPILoader.cat_col_mappings)\n",
        "        #concatenated_inputs = tf.keras.layers.Concatenate(axis=-1)(inputs)\n",
        "        flattened_inputs = tf.keras.layers.Flatten()(tf.keras.layers.concatenate(inputs))\n",
        "        x = tf.keras.layers.Dropout(self.neural_network_structure[\"input_dropout\"])(flattened_inputs)\n",
        "\n",
        "        mean_dnn, sampling_dnn = self.build_dnn_two_towers_with_shared_weights()\n",
        "        mean, sampled = mean_dnn(x), sampling_dnn(x)\n",
        "        output_layer = Dense(1, activation=\"sigmoid\", name=\"sample\")\n",
        "        mean, sampled = output_layer(mean), output_layer(sampled)\n",
        "        outputs = tf.keras.layers.concatenate([mean, sampled], name=\"output\")\n",
        "        #print(f\"Output Shape after DNN: {outputs.shape}\")\n",
        "        self.model = tf.keras.models.Model(input_layer, outputs, name=\"model\")\n",
        "        self.model.compile(optimizer='adam', loss=WeightedBCELossWrapper(),\n",
        "            weighted_metrics=[WeightedBCEMetricWrapper()])\n",
        "        #self.model.summary()\n",
        "        return self.model\n",
        "    def fit(self, X, y, sample_weight=None, epochs=30, validation_split=0.2):\n",
        "        self.model.fit(X, y, sample_weight=None, epochs=epochs, validation_split=0.2, callbacks=[PrintFirstBatchCallback()])\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(X)\n",
        "\n",
        "class PrintFirstBatchCallback(tf.keras.callbacks.Callback):\n",
        "    def on_train_batch_begin(self, batch, logs=None):\n",
        "        if batch == 0:  # Check if it's the first batch\n",
        "            # Accessing model inputs directly, adjust as necessary for your model structure\n",
        "            inputs = self.model.inputs\n",
        "            tf.print(\"First batch inputs:\", inputs, summarize=-1)  # summarize=-1 prints all elements\n",
        "\n",
        "\n",
        "class Postprocessor:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Get the model's prediction probabilities\n",
        "        self.predictions = self.model.predict(X)\n",
        "\n",
        "        # Apply the adjusted threshold to these probabilities to get binary predictions\n",
        "        binary_predictions = (self.predictions > 0.5).astype(int)\n",
        "        return binary_predictions, self.predictions"
      ],
      "metadata": {
        "id": "0WqndU3dN48v"
      },
      "id": "0WqndU3dN48v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Valuator:\n",
        "    def __init__(self,preprocessor,model,postprocessor):\n",
        "        #TODO: implement adjusted_threshold here\n",
        "        self.preprocessor=preprocessor\n",
        "        self.model = model\n",
        "        self.postprocessor = postprocessor\n",
        "\n",
        "    def predict(self, data):\n",
        "        # Preprocess data to get it in the correct format\n",
        "        preprocessed_data = self.preprocessor.preprocess_(data, return_xy = False) # Ignore y during prediction\n",
        "\n",
        "        # Ensure X is a list of arrays matching the model's input structure\n",
        "\n",
        "        return self.postprocessor.predict(preprocessed_data)"
      ],
      "metadata": {
        "id": "sex1a5QYUglv"
      },
      "id": "sex1a5QYUglv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "numerical_columns = ['startCount', 'viewCount', 'clickCount', 'installCount', 'startCount1d', 'startCount7d']\n",
        "categorical_columns = ['platform','campaignId','sourceGameId']\n",
        "target_column = 'install'\n",
        "def given_data_create_valuator(daily_df,target_column,epochs=20):\n",
        "  # Assuming 'install' is your target column\n",
        "\n",
        "  # Initialize the preprocessor\n",
        "  #def given_data_create_valuator(daily_df,target_column):\n",
        "  preprocessor = DataPreprocessor(numerical_columns=numerical_columns, target_column=target_column,categorical_columns=categorical_columns)\n",
        "  X,y,weights = preprocessor.preprocess_(daily_df,return_xy=True)\n",
        "  # Convert the target to a numeric type, if it's not already\n",
        "  input_shape = pd.DataFrame(X).shape[1]\n",
        "  # Assuming X is your training features DataFrame and you have 6 numerical columns\n",
        "  my_model = ConversionModel(input_shape, numerical_columns,categorical_columns,preprocessor.cat_col_mappings)\n",
        "  my_model.model.fit(X, y, sample_weight=weights, epochs=epochs, validation_split=0.2)\n",
        "  postprocessor = Postprocessor(my_model)\n",
        "  return Valuator(preprocessor,my_model,postprocessor)"
      ],
      "metadata": {
        "id": "43Y6yWSXh6MB"
      },
      "id": "43Y6yWSXh6MB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import resample\n",
        "valuator = given_data_create_valuator(TRAIN_DATA,'install',epochs=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41P3nSbI_mcC",
        "outputId": "d584e960-da39-4f9e-d315-4fe5d7c5b750"
      },
      "id": "41P3nSbI_mcC",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "882/882 [==============================] - 10s 7ms/step - loss: 0.1852 - binary_crossentropy: 0.1852 - val_loss: 0.0678 - val_binary_crossentropy: 0.0678\n",
            "Epoch 2/20\n",
            "882/882 [==============================] - 5s 6ms/step - loss: 0.0639 - binary_crossentropy: 0.0639 - val_loss: 0.0630 - val_binary_crossentropy: 0.0630\n",
            "Epoch 3/20\n",
            "882/882 [==============================] - 4s 5ms/step - loss: 0.0618 - binary_crossentropy: 0.0618 - val_loss: 0.0632 - val_binary_crossentropy: 0.0632\n",
            "Epoch 4/20\n",
            "882/882 [==============================] - 5s 6ms/step - loss: 0.0604 - binary_crossentropy: 0.0604 - val_loss: 0.0639 - val_binary_crossentropy: 0.0639\n",
            "Epoch 5/20\n",
            "882/882 [==============================] - 4s 5ms/step - loss: 0.0591 - binary_crossentropy: 0.0591 - val_loss: 0.0633 - val_binary_crossentropy: 0.0633\n",
            "Epoch 6/20\n",
            "882/882 [==============================] - 4s 5ms/step - loss: 0.0583 - binary_crossentropy: 0.0583 - val_loss: 0.0639 - val_binary_crossentropy: 0.0639\n",
            "Epoch 7/20\n",
            "882/882 [==============================] - 6s 7ms/step - loss: 0.0569 - binary_crossentropy: 0.0569 - val_loss: 0.0657 - val_binary_crossentropy: 0.0657\n",
            "Epoch 8/20\n",
            "882/882 [==============================] - 4s 5ms/step - loss: 0.0558 - binary_crossentropy: 0.0558 - val_loss: 0.0674 - val_binary_crossentropy: 0.0674\n",
            "Epoch 9/20\n",
            "882/882 [==============================] - 4s 5ms/step - loss: 0.0547 - binary_crossentropy: 0.0547 - val_loss: 0.0698 - val_binary_crossentropy: 0.0698\n",
            "Epoch 10/20\n",
            "882/882 [==============================] - 6s 7ms/step - loss: 0.0536 - binary_crossentropy: 0.0536 - val_loss: 0.0736 - val_binary_crossentropy: 0.0736\n",
            "Epoch 11/20\n",
            "882/882 [==============================] - 4s 5ms/step - loss: 0.0523 - binary_crossentropy: 0.0523 - val_loss: 0.0819 - val_binary_crossentropy: 0.0819\n",
            "Epoch 12/20\n",
            "882/882 [==============================] - 4s 5ms/step - loss: 0.0524 - binary_crossentropy: 0.0524 - val_loss: 0.0820 - val_binary_crossentropy: 0.0820\n",
            "Epoch 13/20\n",
            "882/882 [==============================] - 6s 7ms/step - loss: 0.0513 - binary_crossentropy: 0.0513 - val_loss: 0.0821 - val_binary_crossentropy: 0.0821\n",
            "Epoch 14/20\n",
            "882/882 [==============================] - 4s 5ms/step - loss: 0.0506 - binary_crossentropy: 0.0506 - val_loss: 0.0881 - val_binary_crossentropy: 0.0881\n",
            "Epoch 15/20\n",
            "882/882 [==============================] - 5s 6ms/step - loss: 0.0500 - binary_crossentropy: 0.0500 - val_loss: 0.0959 - val_binary_crossentropy: 0.0959\n",
            "Epoch 16/20\n",
            "882/882 [==============================] - 5s 6ms/step - loss: 0.0493 - binary_crossentropy: 0.0493 - val_loss: 0.0995 - val_binary_crossentropy: 0.0995\n",
            "Epoch 17/20\n",
            "882/882 [==============================] - 4s 5ms/step - loss: 0.0488 - binary_crossentropy: 0.0488 - val_loss: 0.1124 - val_binary_crossentropy: 0.1124\n",
            "Epoch 18/20\n",
            "882/882 [==============================] - 5s 6ms/step - loss: 0.0493 - binary_crossentropy: 0.0493 - val_loss: 0.1062 - val_binary_crossentropy: 0.1062\n",
            "Epoch 19/20\n",
            "882/882 [==============================] - 6s 7ms/step - loss: 0.0482 - binary_crossentropy: 0.0482 - val_loss: 0.1093 - val_binary_crossentropy: 0.1093\n",
            "Epoch 20/20\n",
            "882/882 [==============================] - 4s 5ms/step - loss: 0.0478 - binary_crossentropy: 0.0478 - val_loss: 0.1182 - val_binary_crossentropy: 0.1182\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "aquired_data,probs = valuator.predict(TRAFFIC_DATA)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwOm35YhnArr",
        "outputId": "f258cb1b-04da-4d48-edf8-29bb33650ce4"
      },
      "id": "uwOm35YhnArr",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6691/6691 [==============================] - 17s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unseen_data_ = UNSEEN_DATA.drop(columns=['install'])\n",
        "train_data_ = TRAIN_DATA.drop(columns=['install'])\n",
        "# Add OOD column\n",
        "unseen_data_['OOD'] = 1\n",
        "train_data_['OOD'] = 0\n",
        "ood_train_data = pd.concat([train_data_, unseen_data_], ignore_index=True)\n",
        "ood_train_data = ood_train_data.sample(frac=1, random_state=42).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "lvoKtie-6Wpk"
      },
      "id": "lvoKtie-6Wpk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valuator_ood = given_data_create_valuator(ood_train_data,'OOD',epochs=12)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qz3Z0yJuQ2G",
        "outputId": "26ce7904-c0a6-4cc3-df36-a7607bf028b2"
      },
      "id": "3qz3Z0yJuQ2G",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/12\n",
            "14647/14647 [==============================] - 97s 6ms/step - loss: 0.1200 - binary_crossentropy: 0.1200 - val_loss: 0.0982 - val_binary_crossentropy: 0.0982\n",
            "Epoch 2/12\n",
            "14647/14647 [==============================] - 95s 6ms/step - loss: 0.1001 - binary_crossentropy: 0.1001 - val_loss: 0.0962 - val_binary_crossentropy: 0.0962\n",
            "Epoch 3/12\n",
            "14647/14647 [==============================] - 90s 6ms/step - loss: 0.0974 - binary_crossentropy: 0.0974 - val_loss: 0.0956 - val_binary_crossentropy: 0.0956\n",
            "Epoch 4/12\n",
            "14647/14647 [==============================] - 89s 6ms/step - loss: 0.0956 - binary_crossentropy: 0.0956 - val_loss: 0.0953 - val_binary_crossentropy: 0.0953\n",
            "Epoch 5/12\n",
            "14647/14647 [==============================] - 90s 6ms/step - loss: 0.0944 - binary_crossentropy: 0.0944 - val_loss: 0.0945 - val_binary_crossentropy: 0.0945\n",
            "Epoch 6/12\n",
            "14647/14647 [==============================] - 90s 6ms/step - loss: 0.0933 - binary_crossentropy: 0.0933 - val_loss: 0.0931 - val_binary_crossentropy: 0.0931\n",
            "Epoch 7/12\n",
            "14647/14647 [==============================] - 92s 6ms/step - loss: 0.0926 - binary_crossentropy: 0.0926 - val_loss: 0.0939 - val_binary_crossentropy: 0.0939\n",
            "Epoch 8/12\n",
            "14647/14647 [==============================] - 90s 6ms/step - loss: 0.0920 - binary_crossentropy: 0.0920 - val_loss: 0.0935 - val_binary_crossentropy: 0.0935\n",
            "Epoch 9/12\n",
            "14647/14647 [==============================] - 89s 6ms/step - loss: 0.0915 - binary_crossentropy: 0.0915 - val_loss: 0.0934 - val_binary_crossentropy: 0.0934\n",
            "Epoch 10/12\n",
            "14647/14647 [==============================] - 95s 6ms/step - loss: 0.0912 - binary_crossentropy: 0.0912 - val_loss: 0.0931 - val_binary_crossentropy: 0.0931\n",
            "Epoch 11/12\n",
            "14647/14647 [==============================] - 93s 6ms/step - loss: 0.0908 - binary_crossentropy: 0.0908 - val_loss: 0.0934 - val_binary_crossentropy: 0.0934\n",
            "Epoch 12/12\n",
            "14647/14647 [==============================] - 95s 7ms/step - loss: 0.0907 - binary_crossentropy: 0.0907 - val_loss: 0.0942 - val_binary_crossentropy: 0.0942\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2000 x danych niz parameterow rule of thumb"
      ],
      "metadata": {
        "id": "qmO7a76fE4dJ"
      },
      "id": "qmO7a76fE4dJ"
    },
    {
      "cell_type": "code",
      "source": [
        "_,probs_ood = valuator_ood.predict(TRAFFIC_DATA)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkdNOVBakIgv",
        "outputId": "59e6ac6f-9d80-4943-a680-83e2bec0c79e"
      },
      "id": "KkdNOVBakIgv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6691/6691 [==============================] - 16s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Recall@100,Recall@10,Recall@1000\n",
        "Recall = np.hstack([probs,probs_ood,(TRAFFIC_DATA['country']=='US').map(int).values.reshape(-1,1),TRAFFIC_DATA['install'].map(int).values.reshape(-1,1)])\n",
        "print(f\"Ratio = {sum(Recall[:,4])/len(Recall[:,4])}\")\n",
        "print(f\"Recall of IN in MCdropout mean branch: {Recall[Recall[:, 0].argsort()[::-1]][:1000][:,4].sum()}Recall of IN in MCdropout {Recall[Recall[:, 1].argsort()[::-1]][:1000][:,4].sum()},Recall of IN in OOD classifier {Recall[Recall[:, 2].argsort()[::-1]][:1000][:,4].sum()} out of 1000\")\n",
        "print(f\"Recall of IN in MCdropout mean branch: {Recall[Recall[:, 0].argsort()[::-1]][:100][:,4].sum()},Recall of IN in MCdropout {Recall[Recall[:, 1].argsort()[::-1]][:100][:,4].sum()},Recall of IN in OOD classifier {Recall[Recall[:, 2].argsort()[::-1]][:100][:,4].sum()} out of 100\")\n",
        "print(f\"Recall of IN in MCdropout mean branch: {Recall[Recall[:, 0].argsort()[::-1]][:10][:,4].sum()},RRecall of IN in MCdropout {Recall[Recall[:, 1].argsort()[::-1]][:10][:,4].sum()},Recall of IN in OOD classifier {Recall[Recall[:, 2].argsort()[::-1]][:10][:,4].sum()} out of 10\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwv-NpY9kXyb",
        "outputId": "7f1c50f2-f79c-4a2c-ca5f-91eda8408077"
      },
      "id": "nwv-NpY9kXyb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ratio = 0.15059202690394452\n",
            "Recall of IN in MCdropout mean branch: 198.0Recall of IN in MCdropout 205.0,Recall of IN in OOD classifier 63.0 out of 1000\n",
            "Recall of IN in MCdropout mean branch: 19.0,Recall of IN in MCdropout 20.0,Recall of IN in OOD classifier 5.0 out of 100\n",
            "Recall of IN in MCdropout mean branch: 1.0,RRecall of IN in MCdropout 2.0,Recall of IN in OOD classifier 1.0 out of 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_probs = valuator_ood.predict(TRAIN_DATA)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oYkUT8llfDU",
        "outputId": "66bc443f-15b2-482d-8bee-642f8c8fb538"
      },
      "id": "3oYkUT8llfDU",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "49113/49113 [==============================] - 122s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_probs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzIvPAME0bqp",
        "outputId": "fb947545-18d1-4431-c129-9e5701d37e5a"
      },
      "id": "YzIvPAME0bqp",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2.0352405e-21, 7.5745680e-21, 8.0354813e-21, ..., 1.0000000e+00,\n",
              "       1.0000000e+00, 1.0000000e+00], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_DATA.shape,UNSEEN_DATA.shape,TRAFFIC_DATA.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uyn7BpAE9nFN",
        "outputId": "8433c106-0541-4e72-fe66-76d4283fb2c5"
      },
      "id": "Uyn7BpAE9nFN",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1571590, 17), (292936, 17), (214095, 17))"
            ]
          },
          "metadata": {},
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "aquired_data,probs = valuator.predict(TRAFFIC_DATA)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvWvZ5eRobKS",
        "outputId": "03671147-2b0d-44ab-833b-d71f62d8714b"
      },
      "id": "mvWvZ5eRobKS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6691/6691 [==============================] - 9s 1ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#let's take top 30% according to the probs\n",
        "aquired_data = probs.flatten() > np.percentile(probs.flatten(),70)"
      ],
      "metadata": {
        "id": "w7DPmSwZ8PFz"
      },
      "id": "w7DPmSwZ8PFz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def from_valuator_create_ood(which_data_aquired, new_valuator_data, old_training_data, old_unseen_data):\n",
        "  target_column = 'OOD'\n",
        "\n",
        "  if not isinstance(old_unseen_data, pd.DataFrame) or old_unseen_data.empty:\n",
        "      old_unseen_data = pd.DataFrame(columns=new_valuator_data.columns)\n",
        "  # Ensure which_data_aquired is correctly sized to match new_valuator_data\n",
        "  assert len(which_data_aquired) == new_valuator_data.shape[0], \"Mismatch in lengths.\"\n",
        "\n",
        "  N = sum(which_data_aquired.astype(bool))\n",
        "  old_training_data_modified = old_training_data.iloc[N:]\n",
        "  new_train_data = pd.concat([old_training_data_modified, new_valuator_data[which_data_aquired.astype(bool)]])\n",
        "  new_unseen_data = pd.concat([old_unseen_data, new_valuator_data[~which_data_aquired.astype(bool)]])\n",
        "  print(f\"new_train_data.shape: {new_train_data.shape}, new_unseen_data.shape: {new_unseen_data.shape}\")\n",
        "\n",
        "  print(f\"new_train_data.shape: {new_train_data.shape},new_unseen_data.shape: {new_unseen_data.shape}\")\n",
        "  unseen_data_ = new_unseen_data.drop(columns=['install'])\n",
        "  train_data_ = new_train_data.drop(columns=['install'])\n",
        "\n",
        "  # Add OOD column\n",
        "  unseen_data_['OOD'] = 0\n",
        "  train_data_['OOD'] = 1\n",
        "\n",
        "  ood_train_data = pd.concat([train_data_, unseen_data_], ignore_index=True)\n",
        "\n",
        "  preprocessor_OOD = DataPreprocessor(numerical_columns=numerical_columns, target_column=target_column)\n",
        "  preprocessor_OOD.preprocess_train(ood_train_data)\n",
        "\n",
        "  # Preprocess your DataFrame\n",
        "  X = preprocessor_OOD.balanced_df.drop(columns=[target_column])\n",
        "  y = preprocessor_OOD.balanced_df[target_column]\n",
        "\n",
        "  # Convert the target to a numeric type, if it's not already\n",
        "  y = pd.to_numeric(y, errors='coerce')\n",
        "  input_shape = X.shape[1]\n",
        "\n",
        "  # Initialize and train the model\n",
        "  OOD_model = OODModel(input_shape)\n",
        "  OOD_model.fit(X, y,epochs=20)\n",
        "  postprocessor_OOD = Postprocessor(OOD_model)\n",
        "\n",
        "  return Valuator(preprocessor_OOD,OOD_model,postprocessor_OOD), new_train_data, new_unseen_data"
      ],
      "metadata": {
        "id": "AgNq-_sh-kVI"
      },
      "id": "AgNq-_sh-kVI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's add aquired data to TRAIN_DATA,"
      ],
      "metadata": {
        "id": "VJPiYTywqya0"
      },
      "id": "VJPiYTywqya0"
    },
    {
      "cell_type": "code",
      "source": [
        "valuator_ood, TRAIN_DATA, UNSEEN_DATA = from_valuator_create_ood(aquired_data,TRAFFIC_DATA, TRAIN_DATA, UNSEEN_DATA)"
      ],
      "metadata": {
        "id": "HAp38GKeJeWE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fb789c3-323a-4c23-8785-a85937f2e53d"
      },
      "id": "HAp38GKeJeWE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "new_train_data.shape: (1571590, 17), new_unseen_data.shape: (442802, 17)\n",
            "new_train_data.shape: (1571590, 17),new_unseen_data.shape: (442802, 17)\n",
            "Epoch 1/2\n",
            "22141/22141 [==============================] - 65s 3ms/step - loss: 0.6629 - accuracy: 0.6307 - val_loss: 0.9654 - val_accuracy: 0.1324\n",
            "Epoch 2/2\n",
            "22141/22141 [==============================] - 58s 3ms/step - loss: 0.6471 - accuracy: 0.6378 - val_loss: 0.9607 - val_accuracy: 0.1479\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valuator = given_data_create_valuator(TRAIN_DATA)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoHeL8zBBz-v",
        "outputId": "a5757def-5a68-4eb5-dc8f-42dd2dc17b94"
      },
      "id": "AoHeL8zBBz-v",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "911/911 [==============================] - 4s 3ms/step - loss: 0.1677 - binary_crossentropy: 0.1677 - val_loss: 0.0403 - val_binary_crossentropy: 0.0403\n",
            "Epoch 2/30\n",
            "911/911 [==============================] - 2s 2ms/step - loss: 0.1175 - binary_crossentropy: 0.1175 - val_loss: 0.0311 - val_binary_crossentropy: 0.0311\n",
            "Epoch 3/30\n",
            "911/911 [==============================] - 2s 2ms/step - loss: 0.1052 - binary_crossentropy: 0.1052 - val_loss: 0.0193 - val_binary_crossentropy: 0.0193\n",
            "Epoch 4/30\n",
            "911/911 [==============================] - 2s 2ms/step - loss: 0.1029 - binary_crossentropy: 0.1029 - val_loss: 0.0235 - val_binary_crossentropy: 0.0235\n",
            "Epoch 5/30\n",
            "911/911 [==============================] - 3s 3ms/step - loss: 0.1007 - binary_crossentropy: 0.1007 - val_loss: 0.0217 - val_binary_crossentropy: 0.0217\n",
            "Epoch 6/30\n",
            "911/911 [==============================] - 3s 4ms/step - loss: 0.0995 - binary_crossentropy: 0.0995 - val_loss: 0.0226 - val_binary_crossentropy: 0.0226\n",
            "Epoch 7/30\n",
            "911/911 [==============================] - 3s 3ms/step - loss: 0.0981 - binary_crossentropy: 0.0981 - val_loss: 0.0166 - val_binary_crossentropy: 0.0166\n",
            "Epoch 8/30\n",
            "911/911 [==============================] - 2s 2ms/step - loss: 0.0978 - binary_crossentropy: 0.0978 - val_loss: 0.0199 - val_binary_crossentropy: 0.0199\n",
            "Epoch 9/30\n",
            "911/911 [==============================] - 2s 2ms/step - loss: 0.0978 - binary_crossentropy: 0.0978 - val_loss: 0.0185 - val_binary_crossentropy: 0.0185\n",
            "Epoch 10/30\n",
            "911/911 [==============================] - 3s 3ms/step - loss: 0.0977 - binary_crossentropy: 0.0977 - val_loss: 0.0243 - val_binary_crossentropy: 0.0243\n",
            "Epoch 11/30\n",
            "911/911 [==============================] - 4s 4ms/step - loss: 0.0971 - binary_crossentropy: 0.0971 - val_loss: 0.0160 - val_binary_crossentropy: 0.0160\n",
            "Epoch 12/30\n",
            "911/911 [==============================] - 2s 2ms/step - loss: 0.0977 - binary_crossentropy: 0.0977 - val_loss: 0.0198 - val_binary_crossentropy: 0.0198\n",
            "Epoch 13/30\n",
            "911/911 [==============================] - 2s 2ms/step - loss: 0.0982 - binary_crossentropy: 0.0982 - val_loss: 0.0215 - val_binary_crossentropy: 0.0215\n",
            "Epoch 14/30\n",
            "911/911 [==============================] - 2s 2ms/step - loss: 0.0981 - binary_crossentropy: 0.0981 - val_loss: 0.0193 - val_binary_crossentropy: 0.0193\n",
            "Epoch 15/30\n",
            "911/911 [==============================] - 2s 2ms/step - loss: 0.0974 - binary_crossentropy: 0.0974 - val_loss: 0.0179 - val_binary_crossentropy: 0.0179\n",
            "Epoch 16/30\n",
            "911/911 [==============================] - 3s 4ms/step - loss: 0.0980 - binary_crossentropy: 0.0980 - val_loss: 0.0205 - val_binary_crossentropy: 0.0205\n",
            "Epoch 17/30\n",
            "911/911 [==============================] - 2s 2ms/step - loss: 0.0978 - binary_crossentropy: 0.0978 - val_loss: 0.0221 - val_binary_crossentropy: 0.0221\n",
            "Epoch 18/30\n",
            "911/911 [==============================] - 2s 2ms/step - loss: 0.0975 - binary_crossentropy: 0.0975 - val_loss: 0.0199 - val_binary_crossentropy: 0.0199\n",
            "Epoch 19/30\n",
            "911/911 [==============================] - 2s 2ms/step - loss: 0.0973 - binary_crossentropy: 0.0973 - val_loss: 0.0212 - val_binary_crossentropy: 0.0212\n",
            "Epoch 20/30\n",
            "911/911 [==============================] - 2s 3ms/step - loss: 0.0973 - binary_crossentropy: 0.0973 - val_loss: 0.0245 - val_binary_crossentropy: 0.0245\n",
            "Epoch 21/30\n",
            "911/911 [==============================] - 3s 3ms/step - loss: 0.0973 - binary_crossentropy: 0.0973 - val_loss: 0.0188 - val_binary_crossentropy: 0.0188\n",
            "Epoch 22/30\n",
            "911/911 [==============================] - 3s 4ms/step - loss: 0.0975 - binary_crossentropy: 0.0975 - val_loss: 0.0179 - val_binary_crossentropy: 0.0179\n",
            "Epoch 23/30\n",
            "911/911 [==============================] - 2s 2ms/step - loss: 0.0970 - binary_crossentropy: 0.0970 - val_loss: 0.0216 - val_binary_crossentropy: 0.0216\n",
            "Epoch 24/30\n",
            "911/911 [==============================] - 2s 2ms/step - loss: 0.0976 - binary_crossentropy: 0.0976 - val_loss: 0.0195 - val_binary_crossentropy: 0.0195\n",
            "Epoch 25/30\n",
            "911/911 [==============================] - 2s 2ms/step - loss: 0.0987 - binary_crossentropy: 0.0987 - val_loss: 0.0198 - val_binary_crossentropy: 0.0198\n",
            "Epoch 26/30\n",
            "911/911 [==============================] - 2s 3ms/step - loss: 0.0970 - binary_crossentropy: 0.0970 - val_loss: 0.0228 - val_binary_crossentropy: 0.0228\n",
            "Epoch 27/30\n",
            "911/911 [==============================] - 3s 4ms/step - loss: 0.0969 - binary_crossentropy: 0.0969 - val_loss: 0.0205 - val_binary_crossentropy: 0.0205\n",
            "Epoch 28/30\n",
            "911/911 [==============================] - 2s 3ms/step - loss: 0.0974 - binary_crossentropy: 0.0974 - val_loss: 0.0215 - val_binary_crossentropy: 0.0215\n",
            "Epoch 29/30\n",
            "911/911 [==============================] - 2s 2ms/step - loss: 0.0969 - binary_crossentropy: 0.0969 - val_loss: 0.0189 - val_binary_crossentropy: 0.0189\n",
            "Epoch 30/30\n",
            "911/911 [==============================] - 2s 2ms/step - loss: 0.0980 - binary_crossentropy: 0.0980 - val_loss: 0.0257 - val_binary_crossentropy: 0.0257\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have available OOD and Conversion model to proceed with logic\n"
      ],
      "metadata": {
        "id": "7GWMIPEFr6I0"
      },
      "id": "7GWMIPEFr6I0"
    },
    {
      "cell_type": "code",
      "source": [
        "TRAFFIC_DATA = daily_dfs[9]"
      ],
      "metadata": {
        "id": "fccwq_mfBiow"
      },
      "id": "fccwq_mfBiow",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs = valuator.predict(TRAFFIC_DATA)[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0s2xGW2Br1z",
        "outputId": "b9859940-fbd0-4103-c7e3-cb3b7cad5a9f"
      },
      "id": "S0s2xGW2Br1z",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6604/6604 [==============================] - 10s 1ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "probs_ood = valuator_ood.predict(TRAFFIC_DATA)[1]"
      ],
      "metadata": {
        "id": "5MhNLmvGpz_K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54d88f38-d52a-4dff-8903-5b3bac9014e4"
      },
      "id": "5MhNLmvGpz_K",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6604/6604 [==============================] - 10s 1ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "dane ze stanow powinny byc odchylone w ocenie valuator_ood. w mc dropout wylosujemy top 100 probek i policzyc wariancje - to jest kryterium. porownanie takiego rankingu vs OOD. Jesli wezmiemy recall@100, to oczekujemy ze ood bedzie mialo wiecej USA"
      ],
      "metadata": {
        "id": "vopAD6FgIwvp"
      },
      "id": "vopAD6FgIwvp"
    },
    {
      "cell_type": "code",
      "source": [
        "#With OOD we would select:\n",
        "Recall = np.hstack([probs,probs_ood,(TRAFFIC_DATA['country']=='US').map(int).values.reshape(-1,1)])\n",
        "Recall[Recall[:, 1].argsort()][:100][:,2].sum(),sum(Recall[:,2])/len(Recall[:,2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FM8dCW70tC0X",
        "outputId": "ca1c110e-5462-4c89-b16e-fa93ef1c9695"
      },
      "id": "FM8dCW70tC0X",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24.0, 0.14911487883706295)"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#With MCDropout we'd select:\n",
        "Recall[Recall[:, 0].argsort()][:100][:,2].sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUcLbyY8w56h",
        "outputId": "8dfd11ba-831a-4a97-f40f-5123802a1693"
      },
      "id": "aUcLbyY8w56h",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21.0"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fNKqWZtH0XmK"
      },
      "id": "fNKqWZtH0XmK",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "metadata": {
      "interpreter": {
        "hash": "15c3220e82aafb8408901b964fa2a57de26e04db32fe34c7748a0f76eb189db1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}