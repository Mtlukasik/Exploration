{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mtlukasik/Exploration/blob/main/unity_ds_ui_quiz.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TigNdgTD0qqo"
      },
      "source": [
        "## Import libraries\n",
        "\n",
        "- Import additional libraries of your choice.\n",
        "\n",
        "Although you are expected to demonstrate understanding of ML/DS/statistics tools, a particular choice of libraries and frameworks will not affect evaluation of the solution."
      ],
      "id": "TigNdgTD0qqo"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "A4FMNn8o0qqp"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "import seaborn as sns\n",
        "import keras\n",
        "import sklearn\n",
        "import hashlib\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats as stats\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from keras.layers import Input, Dense, Reshape\n",
        "# Import additional libraries of your choice"
      ],
      "id": "A4FMNn8o0qqp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeYfD33i0qqk"
      },
      "source": [
        "# Unity Data Science quiz\n",
        "\n",
        "At Unity, we develop deep learning models for real-time ads bidding ([OpenRTB](https://www.iab.com/guidelines/openrtb/)) at various ad\n",
        "exchanges. To bid for an ad impression, we estimate the optimal bid value using predicted\n",
        "install probability of campaign together with several other factors e.g. cost per install.\n",
        "\n",
        "In this homework, your task is to **train a model to predict *install probabilities* for ad impressions included in\n",
        "the test data**. In the production environment, the model predictions are used for deriving the optimial bids for available ad campaigns.  The best ad campaign will be shown to the user. Overestimation of install probabilities will lead to unnecessarily high bids and monetary losses, while underestimation of install probabilities will lead to unnecessarily low bids and loss of opportunities for Unity to win ad impressions. Therefore, it is important for the model predictions to be as accurate as possible.\n",
        "\n",
        "## Instructions\n",
        "\n",
        "- Complete the homework using Python and libraries of your choice.\n",
        "- Follow the instructions in this notebook.\n",
        "- Keep the code clean and organized.\n",
        "\n",
        "## Evaluation\n",
        "\n",
        "We focus our evaluation on technical proficiency, analytical skills, problem solving, creative thinking as well as ability to communicate clearly. In particular, we will evaluate:\n",
        "- understanding of the problem (e.g. does a delivered solution meet the specification of the task)\n",
        "- quality of discussion and brevity of the report (e.g. comments in this notebook)\n",
        "- quality of EDA\n",
        "- feature handling & preprocessing\n",
        "- modeling approach\n",
        "- model validation and evaluation\n",
        "\n",
        "We will separately evaluate the predicted test set install probabilities (see below). Although considered as part of the evaluation, the final performance is not the key factor and shall not dominate over the above dimensions.\n",
        "\n",
        "## Data description\n",
        "\n",
        "- ```id```: impression id\n",
        "- ```timestamp```: time of the event in UTC ```\n",
        " all installs happened  long ago this game is probably old```\n",
        "- ```campaignId```: id of the advertising campaign (the game being advertised)\n",
        "- ```platform```: device platform\n",
        "- ```softwareVersion```: OS version of the device\n",
        "- ```country```: country of user\n",
        "- ```sourceGameId```: id of the publishing game (the game being played)\n",
        "- ```startCount```: how many times the user has started (any) campaigns\n",
        "- ```viewCount```: how many times the user has viewed (any) campaigns\n",
        "- ```clickCount```: how many times the user has clicked (any) campaigns\n",
        "- ```installCount```: how many times the user has installed games from this ad network\n",
        "- ```lastStart```: last time the user started any campaign\n",
        "- ```startCount1d```: how many times the user has started (any) campaigns within the last 24 hours\n",
        "- ```startCount7d```: how many times the user has started (any) campaigns within the last 7 days\n",
        "- ```connectionType```: internet connection type\n",
        "- ```deviceType```: device model\n",
        "- ```install```: binary indicator if install was observed (install=1) or not (install=0) after impression\n",
        "\n",
        "## Submission\n",
        "\n",
        "- This Jupyter notebook\n",
        "- A CSV file containing the predicted install probabilities of ad impressions in the test data. The file should have the following columns:\n",
        "    - ```id```: ID of ad impression in the test data\n",
        "    - ```install_proba```: Predicted install probability of ad impression\n"
      ],
      "id": "LeYfD33i0qqk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis\n",
        "by Mateusz Łukasik\n",
        " - start date: 2.05\n",
        " - submitted: 6.05"
      ],
      "metadata": {
        "id": "wo8rXNYSsPZU"
      },
      "id": "wo8rXNYSsPZU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Z4btb2n1jsKk"
      },
      "id": "Z4btb2n1jsKk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6m_k3Um_0qqq"
      },
      "source": [
        "## Load and prepare data"
      ],
      "id": "6m_k3Um_0qqq"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "train_df = pd.read_csv(\"/content/drive/MyDrive/training_data.csv\", sep=\";\", parse_dates=True)\n",
        "train_df = train_df[~train_df['install'].isna()]#inputing install label is classification so there is no use in it"
      ],
      "metadata": {
        "id": "Bc8BuoLlfKnr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f116528e-8e36-4828-ac48-440777198c74"
      },
      "id": "Bc8BuoLlfKnr",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_df[~train_df['lastStart'].isna()]"
      ],
      "metadata": {
        "id": "jEEpfWFSMciG"
      },
      "id": "jEEpfWFSMciG",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['lastStart'] = pd.to_datetime(train_df['lastStart'])"
      ],
      "metadata": {
        "id": "tyEjW2dtL-bv"
      },
      "id": "tyEjW2dtL-bv",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_df.sort_values(by=['timestamp']).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "uVwQcpC1Ms47"
      },
      "id": "uVwQcpC1Ms47",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCqggieHY_io",
        "outputId": "23da320e-0ff3-435b-d215-53ad8c1fc567"
      },
      "id": "MCqggieHY_io",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['id', 'timestamp', 'campaignId', 'platform', 'softwareVersion',\n",
              "       'sourceGameId', 'country', 'startCount', 'viewCount', 'clickCount',\n",
              "       'installCount', 'lastStart', 'startCount1d', 'startCount7d',\n",
              "       'connectionType', 'deviceType', 'install'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Ensure 'timestamp' is in datetime format\n",
        "train_df['timestamp'] = pd.to_datetime(train_df['timestamp'])\n",
        "\n",
        "# Group the DataFrame by hour\n",
        "daily_groups = train_df.groupby(pd.Grouper(key='timestamp', freq='D'))\n",
        "\n",
        "# Initialize a list to hold the sub-DataFrames\n",
        "daily_dfs = []\n",
        "\n",
        "# Iterate over each group and add the sub-DataFrame to the list\n",
        "for _, group in daily_groups:\n",
        "    if not group.empty:  # Check if the group is not empty\n",
        "        daily_dfs.append(group)\n",
        "[i.shape for i in daily_dfs]"
      ],
      "metadata": {
        "id": "8CwLX3FteOUU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d7cbf98-6c7f-4d69-cd14-e0109693cf02"
      },
      "id": "8CwLX3FteOUU",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(315482, 17),\n",
              " (313690, 17),\n",
              " (306825, 17),\n",
              " (281678, 17),\n",
              " (232467, 17),\n",
              " (225278, 17),\n",
              " (189106, 17),\n",
              " (201060, 17),\n",
              " (214095, 17),\n",
              " (211327, 17),\n",
              " (223277, 17),\n",
              " (257102, 17),\n",
              " (272655, 17),\n",
              " (219667, 17)]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.utils import resample\n",
        "\n",
        "class DataPreprocessor:\n",
        "    def __init__(self, numerical_columns, target_column):\n",
        "        \"\"\"\n",
        "        Initialize the preprocessor with the list of numerical columns\n",
        "        to keep and convert to floats, and the name of the target column for undersampling.\n",
        "        \"\"\"\n",
        "        self.numerical_columns = numerical_columns\n",
        "        self.target_column = target_column\n",
        "\n",
        "    def preprocess_(self,df):\n",
        "        # Ensure the target column is included for processing\n",
        "        columns = self.numerical_columns + [self.target_column]\n",
        "        processed_df = df[columns].copy()\n",
        "\n",
        "        # Convert numerical columns to float dtype\n",
        "        for col in self.numerical_columns:\n",
        "            processed_df[col] = pd.to_numeric(processed_df[col], errors='coerce').astype(float)\n",
        "        return processed_df\n",
        "\n",
        "    def preprocess_train(self, df):\n",
        "        self.processed_df = preprocess_(df)\n",
        "        # Perform undersampling\n",
        "        self.df_majority = self.processed_df[self.processed_df[self.target_column] == 0]\n",
        "        self.df_minority = self.processed_df[processed_df[self.target_column] != 0]\n",
        "        self.len_ratio = self.df_majority.shape[0]/(self.df_majority.shape[0]+self.df_minority.shape[0])\n",
        "        # Downsample the majority class\n",
        "        df_majority_downsampled = resample(self.df_majority,\n",
        "                                           replace=False,    # sample without replacement\n",
        "                                           n_samples=len(self.df_minority),  # to match minority class size\n",
        "                                           random_state=123) # reproducible results\n",
        "\n",
        "        # Combine minority class with downsampled majority class\n",
        "        self.balanced_df = pd.concat([self.df_minority, df_majority_downsampled])\n",
        "\n",
        "        return processed_df\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "class ConversionModel:\n",
        "    def __init__(self, input_shape):\n",
        "        self.model = self._build_model(input_shape)\n",
        "\n",
        "    def _build_model(self, input_shape):\n",
        "        # Define a simple Sequential model\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(64, activation='relu', input_shape=(input_shape,)),\n",
        "            keras.layers.Dropout(0.1),\n",
        "            tf.keras.layers.Dense(64, activation='relu'),\n",
        "            keras.layers.Dropout(0.1),\n",
        "            tf.keras.layers.Dense(1, activation='sigmoid')  # Assuming binary classification\n",
        "        ])\n",
        "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    def fit(self, X, y, epochs=40, validation_split=0.2):\n",
        "        self.model.fit(X, y, epochs=epochs, validation_split=validation_split)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(X)\n",
        "\n",
        "class OODModel:\n",
        "    def __init__(self, input_shape):\n",
        "        self.model = self._build_model(input_shape)\n",
        "\n",
        "    def _build_model(self, input_shape):\n",
        "        # Define a simple Sequential model\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(64, activation='relu', input_shape=(input_shape,)),\n",
        "            keras.layers.Dropout(0.1),\n",
        "            tf.keras.layers.Dense(64, activation='relu'),\n",
        "            keras.layers.Dropout(0.1),\n",
        "            tf.keras.layers.Dense(1, activation='sigmoid')  # Assuming binary classification\n",
        "        ])\n",
        "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    def fit(self, X, y, epochs=40, validation_split=0.2):\n",
        "        self.model.fit(X, y, epochs=epochs, validation_split=validation_split)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(X)\n",
        "\n",
        "class Postprocessor:\n",
        "    def __init__(self, model, adjusted_threshold):\n",
        "        self.model = model\n",
        "        self.adjusted_threshold = adjusted_threshold\n",
        "        print(self.adjusted_threshold)\n",
        "    def predict(self, X):\n",
        "        # Get the model's prediction probabilities\n",
        "        predictions = self.model.predict(X)\n",
        "\n",
        "        # Apply the adjusted threshold to these probabilities to get binary predictions\n",
        "        binary_predictions = (predictions > self.adjusted_threshold*0.8).astype(int)\n",
        "        return binary_predictions\n",
        "\n",
        "class Valuator:\n",
        "  def __init__(self,preprocessor,model,postprocessor,adjusted_threshold):\n",
        "    #TODO: implement adjusted_threshold here\n",
        "    self.preprocessor=preprocessor\n",
        "    self.model = model\n",
        "    self.postprocessor = postprocessor\n",
        "  def predict(self,data):\n",
        "    return self.postprocessor.predict(self.preprocessor.preprocess_(data).drop(columns=[target_column]))"
      ],
      "metadata": {
        "id": "0WqndU3dN48v"
      },
      "id": "0WqndU3dN48v",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "numerical_columns = ['startCount', 'viewCount', 'clickCount', 'installCount', 'startCount1d', 'startCount7d']\n",
        "\n",
        "def given_data_create_valuator(daily_df):\n",
        "  target_column = 'install'  # Assuming 'install' is your target column\n",
        "\n",
        "  # Initialize the preprocessor\n",
        "  preprocessor = DataPreprocessor(numerical_columns=numerical_columns, target_column=target_column)\n",
        "  preprocessor.preprocess_train(daily_df)\n",
        "  # Preprocess your DataFrame\n",
        "  X = preprocessor.balanced_df.drop(columns=[target_column])\n",
        "  y = preprocessor.balanced_df[target_column]\n",
        "  # Convert the target to a numeric type, if it's not already\n",
        "  y = pd.to_numeric(y, errors='coerce')\n",
        "  input_shape = X.shape[1]\n",
        "  # Initialize and train the model\n",
        "  my_model = ConversionModel(input_shape)\n",
        "  my_model.fit(X, y)\n",
        "  postprocessor = Postprocessor(my_model,preprocessor.len_ratio)\n",
        "  return Valuator(preprocessor,my_model,postprocessor,preprocessor.len_ratio)\n",
        "\n",
        "#Trzeba dotagować Treningowe df[0]\n",
        "def from_valuator_create_ood(aquired_data, daily_df):\n",
        "  unseen_data = daily_df[~aquired_data.astype(bool)]\n",
        "  train_data_ = daily_df[aquired_data.astype(bool)]\n",
        "  unseen_data = unseen_data.drop(columns=['install'])\n",
        "  train_data = train_data_.drop(columns=['install'])\n",
        "  # Add OOD column\n",
        "  unseen_data['OOD'] = 0\n",
        "  train_data['OOD'] = 1\n",
        "  ood_train_data = pd.concat([train_data, unseen_data], ignore_index=True)\n",
        "  target_column = 'OOD'  # Assuming 'install' is your target column\n",
        "  preprocessor_OOD = DataPreprocessor(numerical_columns=numerical_columns, target_column=target_column)\n",
        "  preprocessor_OOD.preprocess_train(ood_train_data)\n",
        "  # Preprocess your DataFrame\n",
        "  X = preprocessor_OOD.balanced_df.drop(columns=[target_column])\n",
        "  y = preprocessor_OOD.balanced_df[target_column]\n",
        "  # Convert the target to a numeric type, if it's not already\n",
        "  y = pd.to_numeric(y, errors='coerce')\n",
        "  input_shape = X.shape[1]\n",
        "  # Initialize and train the model\n",
        "  OOD_model = OODModel(input_shape)\n",
        "  OOD_model.fit(X, y)\n",
        "  postprocessor_OOD = Postprocessor(OOD_model,preprocessor_OOD.len_ratio)\n",
        "  return Valuator(preprocessor_OOD,OOD_model,postprocessor,preprocessor_OOD.len_ratio),train_data_"
      ],
      "metadata": {
        "id": "43Y6yWSXh6MB"
      },
      "id": "43Y6yWSXh6MB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valuator = given_data_create_valuator(daily_dfs[0])\n",
        "aquired_data = valuator.predict(daily_dfs[1]).flatten()\n",
        "valuator_ood, train_data_ = from_valuator_create_ood(aquired_data, daily_dfs[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nbC57nnoeD3",
        "outputId": "076639a1-20a3-4bdd-9f2c-7a3f3ab49a0e"
      },
      "id": "1nbC57nnoeD3",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.970273199655711\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6lW6Aytk0iab"
      },
      "id": "6lW6Aytk0iab",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in enumerate(daily_dfs):\n",
        "  if i[0]>1.0:\n",
        "    print(f\"In aquired data we got:{sum(train_data_['install'])} installs out of {len(train_data_['install'])} which brings conversion {sum(train_data_['install'])/len(train_data_['install'])}\")\n",
        "    TRAIN_DF_UPDATED = pd.concat([valuator.preprocessor.processed_df, train_data_])\n",
        "    print(f\"New length for ith:{i[0]} iteration {TRAIN_DF_UPDATED.shape} now proceeding to train model again:\")\n",
        "    #Prepare new preprocessing\n",
        "    preprocessor = DataPreprocessor(numerical_columns=numerical_columns, target_column=target_column)\n",
        "    balanced_df = preprocessor.preprocess(TRAIN_DF_UPDATED)\n",
        "    X = balanced_df.drop(columns=[target_column])\n",
        "    y = balanced_df[target_column]\n",
        "    y = pd.to_numeric(y, errors='coerce')\n",
        "    input_shape = X.shape[1]\n",
        "    #Model training\n",
        "    my_model = ConversionModel(input_shape)\n",
        "    my_model.fit(X, y)\n",
        "    print(f\"preprocessor.len_ratio: {preprocessor.len_ratio}\")\n",
        "    #Serving model\n",
        "    postprocessor = Postprocessor(my_model,preprocessor.len_ratio)"
      ],
      "metadata": {
        "id": "9GE0-LO_ERs2"
      },
      "id": "9GE0-LO_ERs2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5V48iJmmbVFO"
      },
      "id": "5V48iJmmbVFO",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "metadata": {
      "interpreter": {
        "hash": "15c3220e82aafb8408901b964fa2a57de26e04db32fe34c7748a0f76eb189db1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}